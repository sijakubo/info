[{"categories":null,"content":"Hey, I’m Simon, a software developer since 2008 and an enthusiastic coder. I love solving problems with clean code, and I mainly work on backend development using Java, Kotlin, TypeScript, and Python. As an iSAQB®-certified software architect, I build scalable systems and enjoy experimenting with new technologies. When I’m not coding, I like exploring new technologies or simply enjoying a good cup of coffee. ☕ ","date":"2025-03-28","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"I came across the release notes for Spring AI 0.8.0 and i thought i might give it a try and it’s frighteningly simple to do so. Here is a short sample Project: https://github.com/sijakubo/spring-ai-test You just have to: include the Spring AI dependencies provide an AI token call the Spring AI proxy (e.g. ChatClient, ImageClient) Supported models are: Chat Models OpenAI Azure Open AI Amazon Bedrock Anthropic’s Claude Cohere’s Command AI21 Labs’ Jurassic-2 Meta’s LLama 2 Amazon’s Titan Google Vertex AI Palm - Gemini support coming soon (follow the WIP branch) HuggingFace - access thousands of models, including those from Meta such as Llama2 Ollama - run AI models on your local machine Text-to-image Models OpenAI with DALL-E StabilityAI Embedding Models OpenAI Azure OpenAI Ollama ONNX PostgresML Bedrock Cohere Bedrock Titan Google VertexAI ","date":"2024-03-08","objectID":"/posts/post-11/:0:0","tags":["spring","ai","openai","azure","amazon","google"],"title":"Enhance your Java Spring Applications with AI","uri":"/posts/post-11/"},{"categories":null,"content":"Include Spring AI Dependencies: just add the Maven BOM for the Spring AI to your gradle dependency management and include the actual Spring AI spring boot Starter dependency build.gradle.kts dependencyManagement { imports { mavenBom(\"org.springframework.ai:spring-ai-bom:0.8.0\") } } dependencies { ... implementation(\"org.springframework.ai:spring-ai-openai-spring-boot-starter\") ... } ","date":"2024-03-08","objectID":"/posts/post-11/:0:1","tags":["spring","ai","openai","azure","amazon","google"],"title":"Enhance your Java Spring Applications with AI","uri":"/posts/post-11/"},{"categories":null,"content":"provide an AI token In order to use an AI Service, you’ll have to provide an actual token. You can do so within the application.properties. You need to provide the token for the AI Service to use within your Application. E.g.: spring.ai.openai.api-key=\u003cOPEN_AI_API_KEY\u003e spring.ai.openai.image.api-key=\u003cOPEN_AI_API_KEY\u003e spring.ai.azure.openai.api-key=\u003cAZURE_API_KEY\u003e You also have the option to configure the usage of the designated AI Service. E.g. using the OpenAI Service, you can configure e.g. these options: option description model This is the OpenAI Chat model to use (e.g. ) temperature The sampling temperature to use that controls the apparent creativity of generated completions maxTokens The maximum number of tokens to generate in the chat completion ","date":"2024-03-08","objectID":"/posts/post-11/:0:2","tags":["spring","ai","openai","azure","amazon","google"],"title":"Enhance your Java Spring Applications with AI","uri":"/posts/post-11/"},{"categories":null,"content":"Call the Spring AI proxy In order to actually use the AI Service, you can call the designated Proxy Client. When just using the ChatClient you can prompt the AI questions, just as if you would chat with the AI. E.g. String quote = chatClient.call(\"Tell a random funny 'The Office' quote\"); or String translatedText = chatClient.call(\"Translate the following text from langauge: %s to language: %s. The text is: %s\" .formatted( translationResource.sourceLanguage(), translationResource.targetLanguage(), translationResource.text() )); ","date":"2024-03-08","objectID":"/posts/post-11/:0:3","tags":["spring","ai","openai","azure","amazon","google"],"title":"Enhance your Java Spring Applications with AI","uri":"/posts/post-11/"},{"categories":null,"content":"Conclusion It’s utterly simple to integrate an AI Service into your application. As often, the only limit is your ability to identify useful UseCases for this. Spring provides easy methods to integrate several AI Services by abstracting beyond the actual service APIs which makes it easy to exchange between multiple Services and evaluate their results to your personal needs. ","date":"2024-03-08","objectID":"/posts/post-11/:0:4","tags":["spring","ai","openai","azure","amazon","google"],"title":"Enhance your Java Spring Applications with AI","uri":"/posts/post-11/"},{"categories":null,"content":"I had significant issues integrating DynamoDB into one of our projects. The main challenge was finding suitable documentation, as many sources describe integration with AWS SDK version 1.x. In true Scout’s honor, here’s a brief description of how to integrate DynamoDB using AWS SDK 2.x into a Spring Boot project. In the project, start the following Docker container to have a local DynamoDB for development purposes. Here’s an excerpt from our docker-compose.yml: dynamodb: image: amazon/dynamodb-local container_name: sis-dynamodb ports: - \"8000:8000\" command: \"-jar DynamoDBLocal.jar -sharedDb -dbPath .\" environment: AWS_ACCESS_KEY_ID: 'DUMMYIDEXAMPLE' AWS_SECRET_ACCESS_KEY: 'DUMMYEXAMPLEKEY' AWS_REGION: 'eu-central-1' We use the AWS SDK for integration. Include the following Gradle import: implementation 'software.amazon.awssdk:dynamodb' //general integration implementation 'software.amazon.awssdk:dynamodb-enhanced' //enhanced methods to work with the dynamo in a more ORM manner The following Spring configuration is necessary to connect DynamoDB locally as well as on AWS: @Configuration @EnableConfigurationProperties(AwsDynamoDBConfig.Config.class) public class AwsDynamoDBConfig { private static final String DEV_ACCESS_KEY_ID = \"DUMMYIDEXAMPLE\"; private static final String DEV_SECRET_ACCESS_KEY = \"DUMMYEXAMPLEKEY\"; @Bean @Profile(\"dev\") public DynamoDbClient amazonDynamoDbDev(Config config) { return DynamoDbClient.builder() .credentialsProvider( StaticCredentialsProvider.create(AwsBasicCredentials.create(DEV_ACCESS_KEY_ID, DEV_SECRET_ACCESS_KEY))) .endpointOverride(config.url()) .region(Region.EU_CENTRAL_1) .build(); } @Bean @Profile(\"!dev\") public DynamoDbClient amazonDynamoDbProd() { return DynamoDbClient.create(); } @Bean public DynamoDbEnhancedClient createDynamoDbEnhancedClient(DynamoDbClient client) { return DynamoDbEnhancedClient.builder().dynamoDbClient(client).build(); } @ConfigurationProperties(prefix = \"dynamodb\") public record Config(URI url) {} } Since the DynamoDB schema is schema-less, it is not necessary to predefine the entire schema. However, we need to create a table with keys and possibly secondary indices. For local development: @Service @Slf4j @RequiredArgsConstructor @Profile(\"dev\") public class DynamoDBInitService { private final DynamoDbClient amazonDynamoDB; @PostConstruct public void initialiseTables() { log.info(\"Initialising DynamoDB tables\"); String tableName = SatelliteImage.TABLE_NAME; try { DescribeTableRequest describeTableRequest = DescribeTableRequest.builder().tableName(tableName).build(); DescribeTableResponse response = amazonDynamoDB.describeTable(describeTableRequest); if (TableStatus.ACTIVE.equals(response.table().tableStatus())) { log.info(\"Table {} is active\", tableName); } } catch (ResourceNotFoundException e) { log.info(\"Table {} does not exist\", tableName); log.info(\"Creating table {}\", tableName); CreateTableRequest createTableRequest = CreateTableRequest.builder() .tableName(tableName) .keySchema(KeySchemaElement.builder() .keyType(KeyType.HASH) .attributeName(\"id\") .build()) .attributeDefinitions( AttributeDefinition.builder() .attributeName(\"id\") .attributeType(ScalarAttributeType.S) .build(), AttributeDefinition.builder() .attributeName(\"field_id\") .attributeType(ScalarAttributeType.S) .build(), AttributeDefinition.builder() .attributeName(\"image_from\") .attributeType(ScalarAttributeType.S) .build()) .globalSecondaryIndexes(GlobalSecondaryIndex.builder() .indexName(\"FieldIDImageFromDateIndex\") .keySchema( KeySchemaElement.builder() .keyType(KeyType.HASH) .attributeName(\"field_id\") .build(), KeySchemaElement.builder() .keyType(KeyType.RANGE) .attributeName(\"image_from\") .build()) .projection(Projection.builder() .projectionType(ProjectionType.ALL) .build()) .provisionedThroughput(ProvisionedThroughput.builder() .readCapacityUnits(10L) .writeCapacityUnits(10L) .build()) .build()) .provisionedThroughput(ProvisionedThroughput.builder() .readCapacityUnits(10L) .writeCapacityUnits(10","date":"2024-01-31","objectID":"/posts/post-10/:0:0","tags":["aws","dynamodb","spring","gitlab-ci"],"title":"Integrate DynamoDB in Spring Boot and Gitlab-CI using aws sdk 2.x","uri":"/posts/post-10/"},{"categories":null,"content":"What is CORS? CORS, an abbreviation for “Cross-Origin Resource Sharing,” fundamentally aims to ensure that data is distributed only to “trusted” users. However, this assurance is not server-side but rather on the client side. For instance, if CORS is enabled on a server, it sends additional response headers, allowing the client to verify whether the request is permitted from the current origin domain. Modern browsers block access to data that is not considered “trusted” for the current origin domain. These are the CORS errors that often occur in the browser’s network tab. Using developer tools such as Postman, data can be retrieved from origins not deemed “trusted” because CORS validation is simply ignored. This can occasionally complicate the analysis of such issues. What CORS Headers are there? Here are some of the essential CORS response headers: Access-Control-Allow-Origin: Specifies which origin domains are allowed to access the resource. Typically, the origin domain of the website is specified here. Access-Control-Allow-Methods: Defines the HTTP methods (e.g., GET, POST, PUT) allowed for accessing the resource. Access-Control-Allow-Headers: Indicates which HTTP headers are allowed in a request when accessing the resource. What are CORS Preflight requests? Often, the client initially sends an OPTIONS (HTTP method) request to the server to ensure that further requests are allowed. This is referred to as a “preflight.” This is done to prevent scenarios where a POST / PUT / DELETE request is executed from an untrusted origin. What do I need to remember? CORS is controlled by the backend/server through response headers. Browsers/clients interpret the response headers and may report a CORS error. Additional trusted domains must be specified as “trusted” server-side in the Access-Control-Allow-Origin header. Development tools typically ignore CORS. CORS Preflight is nothing more than a preceding OPTIONS request to perform CORS validation. ","date":"2023-10-11","objectID":"/posts/post-9/:0:0","tags":["cors","http","backend","frontend"],"title":"CORS for dummies","uri":"/posts/post-9/"},{"categories":null,"content":"Generating a Feature and Feature Collection form postgis is rather simple. If you convert a simple GEOMETRY with the st_asgeojson postgis will generate a GeoJSON “geometry”. SELECT st_asgeojson(f.geometry) FROM field f; will result in a single geometry: { \"type\" : \"Polygon\", \"coordinates\" : [...] } If you provide a RECORD (since Postgis 3.0.0), postgis will generate a Feature with all the fields as Properties: SELECT st_asgeojson(f.*) FROM field f; will result in: { \"type\" : \"Feature\", \"geometry\" : { \"type\" : \"Polygon\", \"coordinates\" : [ ... ] }, \"properties\" : { \"id\" : \"052ff36a-4ea7-4307-9bc3-414f49a62163\", \"area_square_meters\" : 29434, \"altitude\" : 40, \"granule_code\" : \"31UGT\", \"creation_date\" : \"2021-10-21T23:32:25.974059\", \"name\" : \"Simple test field\", ... } } Where the field table having the Columns: id, area_square_meters, … This comes in rather handy, if you, e.g. like to export a Feature Collection by some grouping column. Let’s say, we have a farm_id, where all the fields reference to as the owner of the field. SELECT JSON_BUILD_OBJECT( 'type', 'FeatureCollection', 'features', JSON_AGG(st_asgeojson(f.*)::JSONB) ) AS feature_collection FROM field f GROUP BY farm_id; We have to build the actual root node FeatureCollectio by ourselves and just JSON_AGG the specific Features / Geometries of the farm. This Results in a nice Feature Collection: { \"type\" : \"FeatureCollection\", \"features\" : [ { \"type\" : \"Feature\", \"geometry\" : { \"type\" : \"Polygon\", \"coordinates\" : [ ... ] }, \"properties\" : { \"id\" : \"9836ef53-16bd-40d8-bc4c-d2d9d152c167\", \"area_square_meters\" : 215311, ... } }, { \"type\" : \"Feature\", \"geometry\" : { \"type\" : \"Polygon\", \"coordinates\" : [ ... ] }, \"properties\" : { \"id\" : \"ebdec7b2-204f-453b-b59b-f8eacccf2c44\", \"area_square_meters\" : 48207, ... } } ] } The Resulting Feature Collection can then easily be pasted into e.g. https://geojson.io/ where you can have a look at the farms features on a map. By default, st_asgeojson will append all the Field Columns as properties of the Feature. You can of course reduce the Properties by just selecting the relevant Columns: SELECT JSON_BUILD_OBJECT( 'type', 'FeatureCollection', 'features', JSON_AGG(st_asgeojson(f.*)::JSONB) ) AS feature_collection FROM (SELECT geometry, farm_id, id, area_square_meters FROM field) f GROUP BY farm_id; This would only generate the Properties: id, farm_id, area_square_meters ","date":"2023-04-27","objectID":"/posts/post-8/:0:0","tags":["postgres","postgis","geojson","feature","featurecollection"],"title":"Create a Feature Collection from Postgis Geometries using Postgis \u003e= 3.0.0","uri":"/posts/post-8/"},{"categories":null,"content":"PostgreSQL is a powerful and flexible open-source relational database management system. One of its most useful features is the ability to define custom composite types (Since Postgres 11), which are user-defined data types that combine multiple fields of different types into a single logical entity. In this article, we’ll explore the benefits of using composite types in PostgreSQL and how to use them in your database schema. ","date":"2023-03-30","objectID":"/posts/post-7/:0:0","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Create Composite Type To define a composite type in PostgreSQL, you use the CREATE TYPE statement, followed by the name of the type and a list of its component fields and types. For example, here’s how you might define a audit type: CREATE TYPE AUDIT AS ( created_at TIMESTAMPTZ, created_by UUID, updated_at TIMESTAMPTZ, updated_by UUID ); ","date":"2023-03-30","objectID":"/posts/post-7/:0:1","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Create Table using Composite Type Once you’ve defined a composite type, you can use it in your database schema just like any other data type. For example, you could create a table that includes a column of type customer_info: CREATE TABLE customer ( id UUID PRIMARY KEY, audit AUDIT ); ","date":"2023-03-30","objectID":"/posts/post-7/:0:2","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Insert data To insert data into a table that includes a composite type, you can use the ROW constructor syntax, like this: INSERT INTO customer(id, audit.created_at, audit.created_by) VALUES (gen_random_uuid(), NOW(), '36765812-fa07-4227-bb90-e2b6ff00da89'); ","date":"2023-03-30","objectID":"/posts/post-7/:0:3","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Query data To query data from a table that includes a composite type, you can use the dot notation to access individual fields of the composite type, like this: SELECT (audit).created_at, (audit).created_by FROM customer; Note: You have to use the Parenthesis around the composite property, otherwise PostgreSQL tries to interpret it as a own table. ","date":"2023-03-30","objectID":"/posts/post-7/:0:4","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Define constraints on composite types using DOMAIN - Types Composite Types do not support constraints. However, you can create a DOMAIN Type including constraints like: CREATE DOMAIN AUDIT_DOMAIN AS AUDIT CHECK ( (value).created_at IS NOT NULL AND (value).created_by IS NOT NULL ); Now you have to use the DOMAIN Type instead of the composite type CREATE TABLE customer ( id UUID PRIMARY KEY, audit AUDIT_DOMAIN ); Inserts or updates violating this constraint will fail: INSERT INTO customer(id, audit.created_at) VALUES (gen_random_uuid(), NOW()); --- [23514] ERROR: value for domain audit_domain violates check constraint \"audit_domain_check\" ","date":"2023-03-30","objectID":"/posts/post-7/:0:5","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"Use Composite Types in Hibernate New Annotation @Struct will be introduced in Hibernate 6.2 (currently development phase). @Embeddable @Struct(name = \"AUDIT_DOMAIN\") public class Audit { @NotNull private ZonedDateTime createdAt; @NotNull private UUID createdBy; private ZonedDateTime updatedAt; private UUID updatedBy; } See also https://thorben-janssen.com/composite-type-with-hibernate/ for more information on how to use composite Types with Hibernate ","date":"2023-03-30","objectID":"/posts/post-7/:0:6","tags":["postgres","composite","types","schema","hibernate"],"title":"PostgreSQL Composite Types to reduce boilerplate code from you database schema","uri":"/posts/post-7/"},{"categories":null,"content":"When running multiple Server nodes, sharing the same Database, it becomes more and more important to handle backwards compatibility on the database especially during Deployments. When we start da Deployment, a new Server node is being deployed to a cluster. This node will then migrate the Database to it’s designated schema version. If this new version is not backwards compatible, the currently running Server instances will run into several problems, reading or writing to the database. There is no “easy” solution for this problem. I’d love to share a solution we came up with to you Requirement: A join table (from a 1:n relation) should be removed. For a Simple Example let’s say, we have the following Schema: voucher(id) used_voucher(user_id, voucher_id) user(id) Where as a voucher can only be redeemed by a single user. Problem: If we would just remove the join table used_voucher all currently running nodes would fail to read voucher usages and write newly redeemed vouchers Solution: Split up the Deployment / Schema migration into 2 seperat steps: Migration 1: Introduce a new column: voucher(user_id) Update voucher.user_id with the current values from used_voucher Introduce a Trigger which writes the voucher(user_id) when writing used_voucher(user_id, voucher_id) Introduce a Trigger which writes the used_voucher(user_id, voucher_id) when writing voucher(user_id) Migration 2: Remove both Triggers Drop used_voucher(user_id, voucher_id) Examples: Migration 1: -- Update voucher set user_id on insert used_voucher CREATE OR REPLACE FUNCTION add_user_id_to_voucher() RETURNS TRIGGER AS $func$ BEGIN UPDATE voucher v SET user_id = NEW.user_id WHERE v.id = NEW.voucher_id; RETURN NEW; END; $func$ LANGUAGE plpgsql; CREATE TRIGGER tg_add_user_id_to_voucher AFTER INSERT ON used_voucher FOR EACH ROW EXECUTE PROCEDURE add_voucher_id(); -- Insert used_voucher on Insert / Update of voucher CREATE OR REPLACE FUNCTION insert_used_voucher() RETURNS TRIGGER AS $func$ BEGIN IF NEW.user_id IS NOT NULL THEN INSERT INTO used_voucher(voucher_id, user_id) VALUES (NEW.id, NEW.user_id) ON CONFLICT DO NOTHING; END IF; RETURN NEW; END; $func$ LANGUAGE plpgsql; CREATE TRIGGER tg_insert_used_voucher AFTER INSERT OR UPDATE ON voucher FOR EACH ROW EXECUTE PROCEDURE insert_used_voucher(); Migration 2: DROP TRIGGER IF EXISTS tg_add_user_id_to_voucher; DROP FUNCTION IF EXISTS add_user_id_to_voucher; DROP TRIGGER IF EXISTS tg_insert_used_voucher; DROP FUNCTION IF EXISTS insert_used_voucher; DROP TABLE used_voucher; ","date":"2023-01-12","objectID":"/posts/post-6/:0:0","tags":["postgres","db","schema","deployment","zero-downtime"],"title":"Zero downtime deployment with breaking DB-Schema change - by example","uri":"/posts/post-6/"},{"categories":null,"content":"A great way to include all the required resources for integration tests, Testcontainers provides an easy way to start docker containers from within your testcode. This removes the necessity to create test docker-compose.yml files and write README.md code on how to even run a test. Here is a short example using testcontainers with Spring and Spock on a Postgis / Postgres database. @SpringBootTest @AutoConfigureMockMvc @AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE) @ContextConfiguration(initializers = [Initializer.class]) class FieldControllerTest extends IntegrationTest { @Shared public static JdbcDatabaseContainer postgreSQLContainer = new PostgisContainerProvider() .newInstance() .withDatabaseName(\"integration-tests-db\") .withUsername(\"sa\") .withPassword(\"sa\"); static class Initializer implements ApplicationContextInitializer\u003cConfigurableApplicationContext\u003e { void initialize(ConfigurableApplicationContext configurableApplicationContext) { postgreSQLContainer.start() TestPropertyValues.of( \"spring.datasource.url=\" + postgreSQLContainer.getJdbcUrl(), \"spring.datasource.username=\" + postgreSQLContainer.getUsername(), \"spring.datasource.password=\" + postgreSQLContainer.getPassword() ).applyTo(configurableApplicationContext.getEnvironment()); } } @Autowired MockMvc mockMvc @Autowired FieldRepository fieldRepository void setup() { fieldRepository.save(new Field( UUID.fromString('ad68f894-c16b-4953-b577-7cddb3e85ae5'), \"initSampleField\", new Polygon( PositionSequenceBuilders.variableSized(G2D.class) .add(5.8208837124389, 51.0596004663904) .add(5.83490292265498, 51.0571257015788) .add(5.87078646658134, 51.0451607414904) .add(5.79146302423308, 51.0612386272784) .add(5.8208837124389, 51.0596004663904) .toPositionSequence(), CoordinateReferenceSystems.WGS84 ) )) } ... } To integrate testcontainers into your project, you’ll need the following dependencies: gradle: dependencyManagement { imports { mavenBom \"org.testcontainers:testcontainers-bom:${testcontainersVersion}\" } } dependencies { ... testImplementation \"org.testcontainers:spock:1.17.5\" testImplementation \"org.testcontainers:postgresql:1.17.5\" } Happy testing 🎉 ","date":"2022-10-18","objectID":"/posts/post-5/:0:0","tags":["testcontainers","postgres","postgis","spring","spock"],"title":"Testcontainers | Postgres: Postgis with Spring and Spock","uri":"/posts/post-5/"},{"categories":null,"content":"Operators see: https://www.postgresql.org/docs/9.5/functions-json.html Create column using jsonb CREATE TABLE article ( id UUID PRIMARY KEY, properties JSONB NOT NULL ); Insert values INSERT INTO article VALUES (gen_random_uuid(), '{ \"price\": 12.1, \"name\": \"Kugelschreiber\", \"tags\": { \"manufacturer\": \"Siemens\", \"discounted\": true } }') Select value from JSON select properties -\u003e 'price' properties -\u003e\u003e 'price' from article -\u003e will extract the value as JSONB -\u003e\u003e will extract the value as text Compare values SELECT * FROM article WHERE CAST(properties -\u003e\u003e 'price' AS NUMERIC) \u003e 10 Check where value is contained SELECT * FROM article WHERE properties -\u003e 'tags' ? 'discounted' or SELECT * FROM article WHERE jsonb_exists(properties -\u003e 'tags', 'discounted') Check where json is contained SELECT * FROM article WHERE properties -\u003e 'tags' @\u003e '{\"manufacturer\": \"Siemens\"}' ","date":"2021-07-10","objectID":"/posts/post-4/:0:0","tags":["postgres","json","jsonb"],"title":"Postgres JSON beginner Class","uri":"/posts/post-4/"},{"categories":null,"content":"I’m a huge Fan of groovy and therefore always excited about new additions and changes to the groovy lang. Therefore, here’s a list of my personal highlights of the groovy 3 release. The complete release-notes can be found here: Groovy 3 Release Notes: ","date":"2021-07-09","objectID":"/posts/post-3/:0:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"!in Operator 4 !in [5, 6, 19] // true ","date":"2021-07-09","objectID":"/posts/post-3/:1:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"!instanceof Operator LocalDate.now() !instanceof Temporal // false LocalDate.now() !instanceof Instant // true ","date":"2021-07-09","objectID":"/posts/post-3/:2:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"?= Elvis Operator def last = null last ?= 'Doe' last == 'Doe' ","date":"2021-07-09","objectID":"/posts/post-3/:3:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"=== and !== Identical Operators def emp1 = new Employee(name: \"Simon Jakubowski\") def emp2 = new Employee(name: \"Simon Jakubowski\") def emp3 = emp1 emp1 == emp2 // true emp1 === emp2 // false emp1 === emp3 // true ","date":"2021-07-09","objectID":"/posts/post-3/:4:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"safe map, list, array access def emps = [ \"boss\" : [\"joe\"], \"developer\": [\"sja\", \"sro\", \"tti\"] ] emps[\"boss\"] // [\"joe\"] employees[\"boss\"] // throws NPE employees?[\"boss\"] // null ","date":"2021-07-09","objectID":"/posts/post-3/:5:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"Support for lambda expressions + Method References [\"test\", \"arba\"] .stream() .map(String::toUpperCase) .collect(Collectors.toList()) ","date":"2021-07-09","objectID":"/posts/post-3/:6:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"Reduction of the main groovy package org.codehaus.groovy:groovy:3.0.8 org.codehaus.groovy:groovy-json:3.0.8 * groovy.json.JsonSlurper org.codehaus.groovy:groovy-xml:3.0.8 * groovy.xml.XmlSlurper ","date":"2021-07-09","objectID":"/posts/post-3/:7:0","tags":["groovy","groovy3","release"],"title":"Groovy 3 Release Note Hightlights","uri":"/posts/post-3/"},{"categories":null,"content":"Of course you would write the first blog post about creating the actual blog itself. So do I. Create Gatsby blog ","date":"2020-07-09","objectID":"/posts/post-2/:0:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"Install Gatsby First of all, we need to install Gatsby which will generate the static blog pages $ brew install gatsby-cli ","date":"2020-07-09","objectID":"/posts/post-2/:1:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"Create a blog based on a Template $ gatsby new \u003cname_of_the_blog\u003e \u003ctemplate_url\u003e //e.g.: $ gatsby new blog https://github.com/niklasmtj/gatsby-starter-julia ","date":"2020-07-09","objectID":"/posts/post-2/:2:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"Start the blog and customise it The blog gets refreshed automatically when you save the files in your IDE $ gatsby develop Add Version Control (GitHub) When initializing the Gatsby project. An initial commit has been made with the base structure of the project. After the Adjustments and Customization has been, add the project to VSC. In my example i’m unsing Github. So i created a Repository and added the remote of my project to this location: $ git remote add origin https://github.com/sijakubo/info.git // check the remotes $ git remote -v After that, I pushed the changes to master Add CI + Deployment to GitHub Pages ","date":"2020-07-09","objectID":"/posts/post-2/:3:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"Publish Configuration We’re going to add GitHub Actions to build the project and deploy the blog to GithubPages. To build an publish the changes to github pages, there is a neat project for that: $ npm install gh-pages --save-dev gh-pages needs the repository name in order to copy the public files to GitHub Pages. Therefore, add the following to your gatsby-config.js module.exports = { pathPrefix: \"/\u003crepository_name\u003e\", } Then add a deploy script to your package.json: \"scripts\": { \"deploy\": \"gatsby build --prefix-paths \u0026\u0026 gh-pages -d public\" } ","date":"2020-07-09","objectID":"/posts/post-2/:4:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"Add Github Actions In order to automate all the things, we are going to setup a CI process to publish the blog after we have pushed a change to the master branch. Add the following workflow to your project at \u003cproject-root\u003e/.github/workflows/node.js.yml name: Gatsby Publish # Execute the Action on every master push on: push: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: # checkout the latest master - uses: actions/checkout@v2 # run: npm install # run: npm run build -- --prefix-paths # push the public files to the GitHub pages branch - uses: enriikke/gatsby-gh-pages-action@v2 with: access-token: ${{ secrets.ACCESS_TOKEN }} deploy-branch: gh-pages gatsby-args: --prefix-paths Please note, that this will require a ${{ secrets.ACCESS_TOKEN }}. This Token can be generated in your GitHub Settings and needs to be within the GitHub Repository -\u003e Settings -\u003e Secrets. Otherwise, the build step will fail. ","date":"2020-07-09","objectID":"/posts/post-2/:5:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"},{"categories":null,"content":"finished That’s about it. Your blog should now be available on GitHub Pages: https://sijakubo.github.io/info/ ","date":"2020-07-09","objectID":"/posts/post-2/:6:0","tags":["Gatsby","GitHub","GitHub Pages","CI/CD"],"title":"Creating a blog using Gatsby and host it on GitHub Pages in 15 Minutes for free","uri":"/posts/post-2/"}]